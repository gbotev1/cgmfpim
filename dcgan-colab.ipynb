{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "dcgan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1za-VW3mufl"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g53drxVwn2GF"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLhopkA1qQcl"
      },
      "source": [
        "!unzip reddit_memes_dataset.npy.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9bYDUa3yo0g"
      },
      "source": [
        "!rm -r tmp/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Ixll5Pmufo"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torchvision import utils as vutils\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils import data\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.handlers import ModelCheckpoint, Timer\n",
        "from ignite.metrics import RunningAverage"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzRTSq5imugI"
      },
      "source": [
        "# Network Architecture\n",
        "Taken from:\n",
        "https://github.com/pytorch/ignite/blob/master/examples/gan/dcgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnN61RkYmugI"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    \"\"\" A base class for both generator and the discriminator.\n",
        "    Provides a common weight initialization scheme.\n",
        "    \"\"\"\n",
        "    def weights_init(self):\n",
        "        for m in self.modules():\n",
        "            classname = m.__class__.__name__\n",
        "            if \"Conv\" in classname:\n",
        "                m.weight.data.normal_(0.0, 0.02)\n",
        "            elif \"BatchNorm\" in classname:\n",
        "                m.weight.data.normal_(1.0, 0.02)\n",
        "                m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU-WHufPmugL"
      },
      "source": [
        "class Generator(Net):\n",
        "    \"\"\" Generator network.\n",
        "    Args:\n",
        "        nf (int): Number of filters in the second-to-last deconv layer\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim, nf, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = torch.nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            torch.nn.ConvTranspose2d(in_channels=z_dim, out_channels=nf * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            torch.nn.BatchNorm2d(nf * 8),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            # state size. (nf*8) x 4 x 4\n",
        "            torch.nn.ConvTranspose2d(in_channels=nf * 8, out_channels=nf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(nf * 4),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            # state size. (nf*4) x 8 x 8\n",
        "            torch.nn.ConvTranspose2d(in_channels=nf * 4, out_channels=nf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(nf * 2),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            # state size. (nf*2) x 16 x 16\n",
        "            torch.nn.ConvTranspose2d(in_channels=nf * 2, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(nf),\n",
        "            torch.nn.ReLU(inplace=True),\n",
        "            # state size. (nf) x 32 x 32\n",
        "            torch.nn.ConvTranspose2d(in_channels=nf, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "        self.weights_init()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqlO2IF7mugN"
      },
      "source": [
        "class Discriminator(Net):\n",
        "    \"\"\" Discriminator network.\n",
        "    Args:\n",
        "        nf (int): Number of filters in the first conv layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, nc, nf):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = torch.nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            torch.nn.Conv2d(in_channels=nc, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (nf) x 32 x 32\n",
        "            torch.nn.Conv2d(in_channels=nf, out_channels=nf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(nf * 2),\n",
        "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (nf*2) x 16 x 16\n",
        "            torch.nn.Conv2d(in_channels=nf * 2, out_channels=nf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(nf * 4),\n",
        "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (nf*4) x 8 x 8\n",
        "            torch.nn.Conv2d(in_channels=nf * 4, out_channels=nf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm2d(nf * 8),\n",
        "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (nf*8) x 4 x 4\n",
        "            torch.nn.Conv2d(in_channels=nf * 8, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            torch.nn.Sigmoid(),\n",
        "        )\n",
        "        self.weights_init()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.net(x)\n",
        "        return output.view(-1, 1).squeeze(1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHumvsUcmugP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-8vxTqpmugP"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le5uM3GsmugP"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 554\n",
        "alpha = 0.98  # Smoothing constant for exponential moving averages\n",
        "num_channels = 3  # Number of image channels\n",
        "z_dim = 100  # Size of latent z vector\n",
        "g_filters = 64  # Number of filters in the second-to-last generator deconv layer\n",
        "d_filters = 64  # Number of filters in first discriminator conv layer\n",
        "learning_rate = 0.001\n",
        "FAKE_IMG_FNAME = 'fake_sample_epoch_{:04d}.png'\n",
        "REAL_IMG_FNAME = 'real_sample_epoch_{:04d}.png'\n",
        "LOGS_FNAME = 'logs.tsv'\n",
        "PLOT_FNAME = 'plot.svg'\n",
        "SAMPLES_FNAME = 'samples.svg'\n",
        "output_dir = 'tmp'\n",
        "epochs = 2500"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yyv0rfemugR"
      },
      "source": [
        "Set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkeLFJJqrNen"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, tensors):\n",
        "        self.data = tensors\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckqUVJIGmugS"
      },
      "source": [
        "dataset = CustomDataset(torch.tensor(np.load('reddit_memes_dataset.npy')))\n",
        "loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "netG = Generator(z_dim, g_filters, num_channels).to(device)\n",
        "netD = Discriminator(num_channels, d_filters).to(device)\n",
        "\n",
        "bce = torch.nn.BCELoss()\n",
        "\n",
        "optimizerG = torch.optim.AdamW(netG.parameters(), lr=learning_rate)\n",
        "optimizerD = torch.optim.AdamW(netD.parameters(), lr=learning_rate)\n",
        "\n",
        "real_labels = torch.ones(batch_size, device=device)\n",
        "fake_labels = torch.zeros(batch_size, device=device)\n",
        "fixed_noise = torch.randn(batch_size, z_dim, 1, 1, device=device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgMCb9X7mugV"
      },
      "source": [
        "def get_noise():\n",
        "    return torch.randn(batch_size, z_dim, 1, 1, device=device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9uEjNw0mugX",
        "outputId": "55a9c714-52af-46b4-f8cc-23bf5574b668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The main function, processing a batch of examples\n",
        "def step(engine, real):\n",
        "    real = real.to(device)\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "    netD.zero_grad()\n",
        "\n",
        "    # train with real\n",
        "    output = netD(real)\n",
        "    errD_real = bce(output, real_labels)\n",
        "    D_x = output.mean().item()\n",
        "\n",
        "    errD_real.backward()\n",
        "\n",
        "    # get fake image from generator\n",
        "    noise = get_noise()\n",
        "    fake = netG(noise)\n",
        "\n",
        "    # train with fake\n",
        "    output = netD(fake.detach())\n",
        "    errD_fake = bce(output, fake_labels)\n",
        "    D_G_z1 = output.mean().item()\n",
        "\n",
        "    errD_fake.backward()\n",
        "\n",
        "    # gradient update\n",
        "    errD = errD_real + errD_fake\n",
        "    optimizerD.step()\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # (2) Update G network: maximize log(D(G(z)))\n",
        "    netG.zero_grad()\n",
        "\n",
        "    # Update generator. We want to make a step that will make it more likely that discriminator outputs \"real\"\n",
        "    output = netD(fake)\n",
        "    errG = bce(output, real_labels)\n",
        "    D_G_z2 = output.mean().item()\n",
        "\n",
        "    errG.backward()\n",
        "\n",
        "    # gradient update\n",
        "    optimizerG.step()\n",
        "\n",
        "    return {'errD': errD.item(), \n",
        "            'errG': errG.item(), \n",
        "            'D_x': D_x,\n",
        "            'D_G_z1': D_G_z1, \n",
        "            'D_G_z2': D_G_z2}\n",
        "\n",
        "# Ignite objects\n",
        "trainer = Engine(step)\n",
        "checkpoint_handler = ModelCheckpoint(output_dir, 'networks', n_saved=1, require_empty=False)\n",
        "timer = Timer(average=True)\n",
        "\n",
        "# Attach running average metrics\n",
        "monitoring_metrics = ['errD', 'errG', 'D_x', 'D_G_z1', 'D_G_z2']\n",
        "RunningAverage(alpha=alpha, output_transform=lambda x: x['errD']).attach(trainer, 'errD')\n",
        "RunningAverage(alpha=alpha, output_transform=lambda x: x['errG']).attach(trainer, 'errG')\n",
        "RunningAverage(alpha=alpha, output_transform=lambda x: x['D_x']).attach(trainer, 'D_x')\n",
        "RunningAverage(alpha=alpha, output_transform=lambda x: x['D_G_z1']).attach(trainer, 'D_G_z1')\n",
        "RunningAverage(alpha=alpha, output_transform=lambda x: x['D_G_z2']).attach(trainer, 'D_G_z2')\n",
        "\n",
        "@trainer.on(Events.ITERATION_COMPLETED(every=1))\n",
        "def print_logs(engine):\n",
        "    fname = os.path.join(output_dir, LOGS_FNAME)\n",
        "    columns = ['iteration',] + list(engine.state.metrics.keys())\n",
        "    values = [str(engine.state.iteration),] + [str(round(value, 5)) for value in engine.state.metrics.values()]\n",
        "    with open(fname, 'a') as f:\n",
        "        if f.tell() == 0:\n",
        "            print('\\t'.join(columns), file=f)\n",
        "        print('\\t'.join(values), file=f)\n",
        "\n",
        "# Adding handlers using `trainer.on` decorator API\n",
        "@trainer.on(Events.EPOCH_COMPLETED(every=25))\n",
        "def save_fake_example(engine):\n",
        "    fake = netG(fixed_noise)\n",
        "    path = os.path.join(output_dir, FAKE_IMG_FNAME.format(engine.state.epoch))\n",
        "    vutils.save_image(fake.detach(), path, normalize=True)\n",
        "\n",
        "# Adding handlers using `trainer.on` decorator API\n",
        "@trainer.on(Events.EPOCH_COMPLETED(every=25))\n",
        "def save_real_example(engine):\n",
        "    img = engine.state.batch\n",
        "    path = os.path.join(output_dir, REAL_IMG_FNAME.format(engine.state.epoch))\n",
        "    vutils.save_image(img, path, normalize=True)\n",
        "\n",
        "# Adding handlers using `trainer.add_event_handler` method API\n",
        "trainer.add_event_handler(\n",
        "    event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={'netG': netG, 'netD': netD}\n",
        ")\n",
        "\n",
        "# Automatically adding handlers via a special `attach` method of `Timer` handler\n",
        "timer.attach(\n",
        "    trainer,\n",
        "    start=Events.EPOCH_STARTED,\n",
        "    resume=Events.ITERATION_STARTED,\n",
        "    pause=Events.ITERATION_COMPLETED,\n",
        "    step=Events.ITERATION_COMPLETED,\n",
        ")\n",
        "\n",
        "# Adding handlers using `trainer.on` decorator API\n",
        "@trainer.on(Events.EPOCH_COMPLETED(every=25))\n",
        "def print_times(engine):\n",
        "    print('Epoch {} done. Time per batch: {:.3f}[s]'.format(engine.state.epoch, timer.value()))\n",
        "    timer.reset()\n",
        "\n",
        "# Adding handlers using `trainer.on` decorator API\n",
        "@trainer.on(Events.EXCEPTION_RAISED)\n",
        "def handle_exception(engine, e):\n",
        "    if isinstance(e, KeyboardInterrupt) and (engine.state.iteration > 1):\n",
        "        engine.terminate()\n",
        "        warnings.warn('KeyboardInterrupt caught. Exiting gracefully.')\n",
        "\n",
        "        create_plots(engine)\n",
        "        checkpoint_handler(engine, {'netG_exception': netG, 'netD_exception': netD})\n",
        "\n",
        "    else:\n",
        "        raise e\n",
        "\n",
        "# Setup is done. Now let's run the training\n",
        "trainer.run(loader, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 25 done. Time per batch: 0.432[s]\n",
            "Epoch 50 done. Time per batch: 0.433[s]\n",
            "Epoch 75 done. Time per batch: 0.433[s]\n",
            "Epoch 100 done. Time per batch: 0.435[s]\n",
            "Epoch 125 done. Time per batch: 0.435[s]\n",
            "Epoch 150 done. Time per batch: 0.435[s]\n",
            "Epoch 175 done. Time per batch: 0.433[s]\n",
            "Epoch 200 done. Time per batch: 0.434[s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gg9t5ymyGbf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}