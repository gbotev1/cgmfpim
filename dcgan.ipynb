{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from pickle import HIGHEST_PROTOCOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Users/gbotev/Downloads/memes'\n",
    "new_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define `CustomDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Custom dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.img_names = [name for name in os.listdir(root_dir) if os.path.isfile(os.path.join(self.root_dir, name))]\n",
    "        self.num_imgs = len(self.img_names)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.root_dir,\n",
    "                                      self.img_names[index]))\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Normalization and Initialize `CustomDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.6056310534477234\n",
      " Std: 0.2573034465312958\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(root_dir,\n",
    "                        T.Compose([T.Resize(new_size),\n",
    "                                   T.ToTensor()]))\n",
    "means = []\n",
    "stds = []\n",
    "for img in dataset:\n",
    "    means.append(torch.mean(img))\n",
    "    stds.append(torch.std(img))\n",
    "mean = torch.mean(torch.tensor(means))\n",
    "std = torch.mean(torch.tensor(stds))\n",
    "print(f'Mean: {mean}\\n Std: {std}')\n",
    "norm = T.Normalize(mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(root_dir,\n",
    "                        T.Compose([T.Resize(new_size),\n",
    "                                   T.ToTensor(),\n",
    "                                   norm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure we have 3,326 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3326"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "Taken from:\n",
    "https://github.com/pytorch/ignite/blob/master/examples/gan/dcgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\" A base class for both generator and the discriminator.\n",
    "    Provides a common weight initialization scheme.\n",
    "    \"\"\"\n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            classname = m.__class__.__name__\n",
    "            if \"Conv\" in classname:\n",
    "                m.weight.data.normal_(0.0, 0.02)\n",
    "            elif \"BatchNorm\" in classname:\n",
    "                m.weight.data.normal_(1.0, 0.02)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Net):\n",
    "    \"\"\" Generator network.\n",
    "    Args:\n",
    "        nf (int): Number of filters in the second-to-last deconv layer\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, nf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(in_channels=z_dim, out_channels=nf * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(nf * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (nf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(in_channels=nf * 8, out_channels=nf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (nf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(in_channels=nf * 4, out_channels=nf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (nf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(in_channels=nf * 2, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(nf),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size. (nf) x 32 x 32\n",
    "            nn.ConvTranspose2d(in_channels=nf, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "        self.weights_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Net):\n",
    "    \"\"\" Discriminator network.\n",
    "    Args:\n",
    "        nf (int): Number of filters in the first conv layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, nc, nf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(in_channels=nc, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (nf) x 32 x 32\n",
    "            nn.Conv2d(in_channels=nf, out_channels=nf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (nf*2) x 16 x 16\n",
    "            nn.Conv2d(in_channels=nf * 2, out_channels=nf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (nf*4) x 8 x 8\n",
    "            nn.Conv2d(in_channels=nf * 4, out_channels=nf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (nf*8) x 4 x 4\n",
    "            nn.Conv2d(in_channels=nf * 8, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.weights_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_workers = 2\n",
    "loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=n_workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# netowrks\n",
    "netG = Generator(z_dim, g_filters, num_channels).to(device)\n",
    "netD = Discriminator(num_channels, d_filters).to(device)\n",
    "\n",
    "# criterion\n",
    "bce = nn.BCELoss()\n",
    "\n",
    "# optimizers\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta_1, 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta_1, 0.999))\n",
    "\n",
    "# load pre-trained models\n",
    "if saved_G:\n",
    "    netG.load_state_dict(torch.load(saved_G))\n",
    "\n",
    "if saved_D:\n",
    "    netD.load_state_dict(torch.load(saved_D))\n",
    "\n",
    "# misc\n",
    "real_labels = torch.ones(batch_size, device=device)\n",
    "fake_labels = torch.zeros(batch_size, device=device)\n",
    "fixed_noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "\n",
    "def get_noise():\n",
    "    return torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "\n",
    "# The main function, processing a batch of examples\n",
    "def step(engine, batch):\n",
    "\n",
    "    # unpack the batch. It comes from a dataset, so we have <images, labels> pairs. Discard labels.\n",
    "    real, _ = batch\n",
    "    real = real.to(device)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "    netD.zero_grad()\n",
    "\n",
    "    # train with real\n",
    "    output = netD(real)\n",
    "    errD_real = bce(output, real_labels)\n",
    "    D_x = output.mean().item()\n",
    "\n",
    "    errD_real.backward()\n",
    "\n",
    "    # get fake image from generator\n",
    "    noise = get_noise()\n",
    "    fake = netG(noise)\n",
    "\n",
    "    # train with fake\n",
    "    output = netD(fake.detach())\n",
    "    errD_fake = bce(output, fake_labels)\n",
    "    D_G_z1 = output.mean().item()\n",
    "\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # gradient update\n",
    "    errD = errD_real + errD_fake\n",
    "    optimizerD.step()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # (2) Update G network: maximize log(D(G(z)))\n",
    "    netG.zero_grad()\n",
    "\n",
    "    # Update generator. We want to make a step that will make it more likely that discriminator outputs \"real\"\n",
    "    output = netD(fake)\n",
    "    errG = bce(output, real_labels)\n",
    "    D_G_z2 = output.mean().item()\n",
    "\n",
    "    errG.backward()\n",
    "\n",
    "    # gradient update\n",
    "    optimizerG.step()\n",
    "\n",
    "    return {\"errD\": errD.item(), \"errG\": errG.item(), \"D_x\": D_x, \"D_G_z1\": D_G_z1, \"D_G_z2\": D_G_z2}\n",
    "\n",
    "# ignite objects\n",
    "trainer = Engine(step)\n",
    "checkpoint_handler = ModelCheckpoint(output_dir, CKPT_PREFIX, n_saved=10, require_empty=False)\n",
    "timer = Timer(average=True)\n",
    "\n",
    "# attach running average metrics\n",
    "monitoring_metrics = [\"errD\", \"errG\", \"D_x\", \"D_G_z1\", \"D_G_z2\"]\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x[\"errD\"]).attach(trainer, \"errD\")\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x[\"errG\"]).attach(trainer, \"errG\")\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x[\"D_x\"]).attach(trainer, \"D_x\")\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x[\"D_G_z1\"]).attach(trainer, \"D_G_z1\")\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x[\"D_G_z2\"]).attach(trainer, \"D_G_z2\")\n",
    "\n",
    "# attach progress bar\n",
    "pbar = ProgressBar()\n",
    "pbar.attach(trainer, metric_names=monitoring_metrics)\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=PRINT_FREQ))\n",
    "def print_logs(engine):\n",
    "    fname = os.path.join(output_dir, LOGS_FNAME)\n",
    "    columns = [\"iteration\",] + list(engine.state.metrics.keys())\n",
    "    values = [str(engine.state.iteration),] + [str(round(value, 5)) for value in engine.state.metrics.values()]\n",
    "\n",
    "    with open(fname, \"a\") as f:\n",
    "        if f.tell() == 0:\n",
    "            print(\"\\t\".join(columns), file=f)\n",
    "        print(\"\\t\".join(values), file=f)\n",
    "\n",
    "    message = \"[{epoch}/{max_epoch}][{i}/{max_i}]\".format(\n",
    "        epoch=engine.state.epoch, max_epoch=epochs, i=(engine.state.iteration % len(loader)), max_i=len(loader)\n",
    "    )\n",
    "    for name, value in zip(columns, values):\n",
    "        message += \" | {name}: {value}\".format(name=name, value=value)\n",
    "\n",
    "    pbar.log_message(message)\n",
    "\n",
    "# adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_fake_example(engine):\n",
    "    fake = netG(fixed_noise)\n",
    "    path = os.path.join(output_dir, FAKE_IMG_FNAME.format(engine.state.epoch))\n",
    "    vutils.save_image(fake.detach(), path, normalize=True)\n",
    "\n",
    "# adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_real_example(engine):\n",
    "    img, y = engine.state.batch\n",
    "    path = os.path.join(output_dir, REAL_IMG_FNAME.format(engine.state.epoch))\n",
    "    vutils.save_image(img, path, normalize=True)\n",
    "\n",
    "# adding handlers using `trainer.add_event_handler` method API\n",
    "trainer.add_event_handler(\n",
    "    event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={\"netG\": netG, \"netD\": netD}\n",
    ")\n",
    "\n",
    "# automatically adding handlers via a special `attach` method of `Timer` handler\n",
    "timer.attach(\n",
    "    trainer,\n",
    "    start=Events.EPOCH_STARTED,\n",
    "    resume=Events.ITERATION_STARTED,\n",
    "    pause=Events.ITERATION_COMPLETED,\n",
    "    step=Events.ITERATION_COMPLETED,\n",
    ")\n",
    "\n",
    "# adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def print_times(engine):\n",
    "    pbar.log_message(\"Epoch {} done. Time per batch: {:.3f}[s]\".format(engine.state.epoch, timer.value()))\n",
    "    timer.reset()\n",
    "\n",
    "# adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def create_plots(engine):\n",
    "    try:\n",
    "        import matplotlib as mpl\n",
    "\n",
    "        mpl.use(\"agg\")\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    except ImportError:\n",
    "        warnings.warn(\"Loss plots will not be generated -- pandas or matplotlib not found\")\n",
    "\n",
    "    else:\n",
    "        df = pd.read_csv(os.path.join(output_dir, LOGS_FNAME), delimiter=\"\\t\", index_col=\"iteration\")\n",
    "        _ = df.plot(subplots=True, figsize=(20, 20))\n",
    "        _ = plt.xlabel(\"Iteration number\")\n",
    "        fig = plt.gcf()\n",
    "        path = os.path.join(output_dir, PLOT_FNAME)\n",
    "\n",
    "        fig.savefig(path)\n",
    "\n",
    "# adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EXCEPTION_RAISED)\n",
    "def handle_exception(engine, e):\n",
    "    if isinstance(e, KeyboardInterrupt) and (engine.state.iteration > 1):\n",
    "        engine.terminate()\n",
    "        warnings.warn(\"KeyboardInterrupt caught. Exiting gracefully.\")\n",
    "\n",
    "        create_plots(engine)\n",
    "        checkpoint_handler(engine, {\"netG_exception\": netG, \"netD_exception\": netD})\n",
    "\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Setup is done. Now let's run the training\n",
    "trainer.run(loader, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
