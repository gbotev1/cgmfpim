{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torchvision import utils as vutils\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils import data\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import ModelCheckpoint, Timer\n",
    "from ignite.metrics import RunningAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Users/gbotev/Downloads/memes'\n",
    "new_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define `CustomDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Custom dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.img_names = [name for name in os.listdir(root_dir) if os.path.isfile(os.path.join(self.root_dir, name))]\n",
    "        self.num_imgs = len(self.img_names)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.root_dir,\n",
    "                                      self.img_names[index]))\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Normalization and Initialize `CustomDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(root_dir,\n",
    "                        T.Compose([T.Resize((new_size, new_size)),\n",
    "                                   T.ToTensor()]))\n",
    "means = []\n",
    "stds = []\n",
    "for img in dataset:\n",
    "    means.append(torch.mean(img))\n",
    "    stds.append(torch.std(img))\n",
    "mean = torch.mean(torch.tensor(means))\n",
    "std = torch.mean(torch.tensor(stds))\n",
    "print(f'Mean: {mean}\\n Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(root_dir,\n",
    "                        T.Compose([T.Resize((new_size, new_size)),\n",
    "                                   T.ToTensor(),\n",
    "                                   T.Normalize(mean=mean, std=std)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to make sure we have 3,326 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = []\n",
    "for datum in dataset:\n",
    "    to_save.append(datum.numpy())\n",
    "np.save('reddit_memes_dataset.npy', np.array(to_save))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "Taken from:\n",
    "https://github.com/pytorch/ignite/blob/master/examples/gan/dcgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\" A base class for both generator and the discriminator.\n",
    "    Provides a common weight initialization scheme.\n",
    "    \"\"\"\n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            classname = m.__class__.__name__\n",
    "            if \"Conv\" in classname:\n",
    "                m.weight.data.normal_(0.0, 0.02)\n",
    "            elif \"BatchNorm\" in classname:\n",
    "                m.weight.data.normal_(1.0, 0.02)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Net):\n",
    "    \"\"\" Generator network.\n",
    "    Args:\n",
    "        nf (int): Number of filters in the second-to-last deconv layer\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, nf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            torch.nn.ConvTranspose2d(in_channels=z_dim, out_channels=nf * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            torch.nn.BatchNorm2d(nf * 8),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # state size. (nf*8) x 4 x 4\n",
    "            torch.nn.ConvTranspose2d(in_channels=nf * 8, out_channels=nf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(nf * 4),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # state size. (nf*4) x 8 x 8\n",
    "            torch.nn.ConvTranspose2d(in_channels=nf * 4, out_channels=nf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(nf * 2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # state size. (nf*2) x 16 x 16\n",
    "            torch.nn.ConvTranspose2d(in_channels=nf * 2, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(nf),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # state size. (nf) x 32 x 32\n",
    "            torch.nn.ConvTranspose2d(in_channels=nf, out_channels=nc, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            torch.nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "        self.weights_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Net):\n",
    "    \"\"\" Discriminator network.\n",
    "    Args:\n",
    "        nf (int): Number of filters in the first conv layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, nc, nf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            torch.nn.Conv2d(in_channels=nc, out_channels=nf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (nf) x 32 x 32\n",
    "            torch.nn.Conv2d(in_channels=nf, out_channels=nf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(nf * 2),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (nf*2) x 16 x 16\n",
    "            torch.nn.Conv2d(in_channels=nf * 2, out_channels=nf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(nf * 4),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (nf*4) x 8 x 8\n",
    "            torch.nn.Conv2d(in_channels=nf * 4, out_channels=nf * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(nf * 8),\n",
    "            torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (nf*8) x 4 x 4\n",
    "            torch.nn.Conv2d(in_channels=nf * 8, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        self.weights_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 3326\n",
    "alpha = 0.98  # Smoothing constant for exponential moving averages\n",
    "beta_1 = 0.5  # For Adam optimizer\n",
    "num_channels = 3  # Number of image channels\n",
    "z_dim = 100  # Size of latent z vector\n",
    "g_filters = 64  # Number of filters in the second-to-last generator deconv layer\n",
    "d_filters = 64  # Number of filters in first discriminator conv layer\n",
    "learning_rate = 0.0002\n",
    "FAKE_IMG_FNAME = 'fake_sample_epoch_{:04d}.png'\n",
    "REAL_IMG_FNAME = 'real_sample_epoch_{:04d}.png'\n",
    "LOGS_FNAME = 'logs.tsv'\n",
    "PLOT_FNAME = 'plot.svg'\n",
    "SAMPLES_FNAME = 'samples.svg'\n",
    "output_dir = '.'\n",
    "print_every = 4  # Log after this many iterations\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.TensorDataset(torch.tensor(np.load('reddit_memes_dataset.npy')))\n",
    "loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "netG = Generator(z_dim, g_filters, num_channels).to(device)\n",
    "netD = Discriminator(num_channels, d_filters).to(device)\n",
    "\n",
    "bce = torch.nn.BCELoss()\n",
    "\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta_1, 0.999))\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta_1, 0.999))\n",
    "\n",
    "real_labels = torch.ones(batch_size, device=device)\n",
    "fake_labels = torch.zeros(batch_size, device=device)\n",
    "fixed_noise = torch.randn(batch_size, z_dim, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise():\n",
    "    return torch.randn(batch_size, z_dim, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function, processing a batch of examples\n",
    "def step(engine, real):\n",
    "    real = real.to(device)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "    netD.zero_grad()\n",
    "\n",
    "    # train with real\n",
    "    output = netD(real)\n",
    "    errD_real = bce(output, real_labels)\n",
    "    D_x = output.mean().item()\n",
    "\n",
    "    errD_real.backward()\n",
    "\n",
    "    # get fake image from generator\n",
    "    noise = get_noise()\n",
    "    fake = netG(noise)\n",
    "\n",
    "    # train with fake\n",
    "    output = netD(fake.detach())\n",
    "    errD_fake = bce(output, fake_labels)\n",
    "    D_G_z1 = output.mean().item()\n",
    "\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # gradient update\n",
    "    errD = errD_real + errD_fake\n",
    "    optimizerD.step()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # (2) Update G network: maximize log(D(G(z)))\n",
    "    netG.zero_grad()\n",
    "\n",
    "    # Update generator. We want to make a step that will make it more likely that discriminator outputs \"real\"\n",
    "    output = netD(fake)\n",
    "    errG = bce(output, real_labels)\n",
    "    D_G_z2 = output.mean().item()\n",
    "\n",
    "    errG.backward()\n",
    "\n",
    "    # gradient update\n",
    "    optimizerG.step()\n",
    "\n",
    "    return {'errD': errD.item(), \n",
    "            'errG': errG.item(), \n",
    "            'D_x': D_x,\n",
    "            'D_G_z1': D_G_z1, \n",
    "            'D_G_z2': D_G_z2}\n",
    "\n",
    "# Ignite objects\n",
    "trainer = Engine(step)\n",
    "checkpoint_handler = ModelCheckpoint(output_dir, 'networks', n_saved=1, require_empty=False)\n",
    "timer = Timer(average=True)\n",
    "\n",
    "# Attach running average metrics\n",
    "monitoring_metrics = ['errD', 'errG', 'D_x', 'D_G_z1', 'D_G_z2']\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x['errD']).attach(trainer, 'errD')\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x['errG']).attach(trainer, 'errG')\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x['D_x']).attach(trainer, 'D_x')\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x['D_G_z1']).attach(trainer, 'D_G_z1')\n",
    "RunningAverage(alpha=alpha, output_transform=lambda x: x['D_G_z2']).attach(trainer, 'D_G_z2')\n",
    "\n",
    "# Attach progress bar\n",
    "pbar = ProgressBar()\n",
    "pbar.attach(trainer, metric_names=monitoring_metrics)\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=print_every))\n",
    "def print_logs(engine):\n",
    "    fname = os.path.join(output_dir, LOGS_FNAME)\n",
    "    columns = ['iteration',] + list(engine.state.metrics.keys())\n",
    "    values = [str(engine.state.iteration),] + [str(round(value, 5)) for value in engine.state.metrics.values()]\n",
    "\n",
    "    with open(fname, 'a') as f:\n",
    "        if f.tell() == 0:\n",
    "            print('\\t'.join(columns), file=f)\n",
    "        print('\\t'.join(values), file=f)\n",
    "\n",
    "    message = '[{epoch}/{max_epoch}][{i}/{max_i}]'.format(\n",
    "        epoch=engine.state.epoch, max_epoch=epochs, i=(engine.state.iteration % len(loader)), max_i=len(loader)\n",
    "    )\n",
    "    for name, value in zip(columns, values):\n",
    "        message += ' | {name}: {value}'.format(name=name, value=value)\n",
    "\n",
    "    pbar.log_message(message)\n",
    "\n",
    "# Adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_fake_example(engine):\n",
    "    fake = netG(fixed_noise)\n",
    "    path = os.path.join(output_dir, FAKE_IMG_FNAME.format(engine.state.epoch))\n",
    "    vutils.save_image(fake.detach(), path, normalize=True)\n",
    "\n",
    "# Adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def save_real_example(engine):\n",
    "    img = engine.state.batch\n",
    "    path = os.path.join(output_dir, REAL_IMG_FNAME.format(engine.state.epoch))\n",
    "    vutils.save_image(img, path, normalize=True)\n",
    "\n",
    "# Adding handlers using `trainer.add_event_handler` method API\n",
    "trainer.add_event_handler(\n",
    "    event_name=Events.EPOCH_COMPLETED, handler=checkpoint_handler, to_save={\"netG\": netG, \"netD\": netD}\n",
    ")\n",
    "\n",
    "# Automatically adding handlers via a special `attach` method of `Timer` handler\n",
    "timer.attach(\n",
    "    trainer,\n",
    "    start=Events.EPOCH_STARTED,\n",
    "    resume=Events.ITERATION_STARTED,\n",
    "    pause=Events.ITERATION_COMPLETED,\n",
    "    step=Events.ITERATION_COMPLETED,\n",
    ")\n",
    "\n",
    "# Adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def print_times(engine):\n",
    "    pbar.log_message('Epoch {} done. Time per batch: {:.3f}[s]'.format(engine.state.epoch, timer.value()))\n",
    "    timer.reset()\n",
    "\n",
    "# Adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def create_plots(engine):\n",
    "    try:\n",
    "        import matplotlib as mpl\n",
    "\n",
    "        mpl.use('agg')\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "    except ImportError:\n",
    "        warnings.warn('Loss plots will not be generated -- pandas or matplotlib not found')\n",
    "\n",
    "    else:\n",
    "        df = pd.read_csv(os.path.join(output_dir, LOGS_FNAME), delimiter='\\t', index_col='iteration')\n",
    "        _ = df.plot(subplots=True, figsize=(20, 20))\n",
    "        _ = plt.xlabel('Iteration number')\n",
    "        fig = plt.gcf()\n",
    "        path = os.path.join(output_dir, PLOT_FNAME)\n",
    "\n",
    "        fig.savefig(path)\n",
    "\n",
    "# adding handlers using `trainer.on` decorator API\n",
    "@trainer.on(Events.EXCEPTION_RAISED)\n",
    "def handle_exception(engine, e):\n",
    "    if isinstance(e, KeyboardInterrupt) and (engine.state.iteration > 1):\n",
    "        engine.terminate()\n",
    "        warnings.warn('KeyboardInterrupt caught. Exiting gracefully.')\n",
    "\n",
    "        create_plots(engine)\n",
    "        checkpoint_handler(engine, {'netG_exception': netG, 'netD_exception': netD})\n",
    "\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Setup is done. Now let's run the training\n",
    "trainer.run(loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
